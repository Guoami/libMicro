    .o files should probably be separated from generated binaries (obj-sun4u?)

    Move more of the system info data gathering into tattle

    Would be nice to show the platform name (uname -i) if available

    Multiview's "red" can get a little too bright.

    Add a summary suite run

    Add ability to compare various tests within a suite?
        e.g.:
            pipe_fmp1   pipe_pmp1   pipe_smp1   pipe_tmp1 
            pipe_fmp4k  pipe_pmp4k  pipe_smp4k  pipe_tmp4k
            pipe_fmt1   pipe_pmt1   pipe_smt1   pipe_tmt1 
            pipe_fmt4k  pipe_pmt4k  pipe_smt4k  pipe_tmt4k
            pipe_fst1   pipe_pst1   pipe_sst1   pipe_tst1 
            pipe_fst4k  pipe_pst4k  pipe_sst4k  pipe_tst4k
    	There are others ...

    Consider testing alternative malloc implementations?

    Add shared vs. private mmap/munmap/mprotect/msync tests?

    Add mutex benchmark tests for the -h switch

    Fold OpenBSD, NetBSD and Android patches for libMicro into the main fold

    Rename suite templates from .cpp to some other name that does not indicate
    they are c++ files

    Refactor barrier code to share more common code, and separate out the
    barrier implementation from the statistics gathering code (and data).

    Investigate why we calculate the values from given batch of runs when we
    have multiple threads and/or processes by finding the minimum start time
    and maximum end time, and then dividing that range by the total number of
    benchmark calls across all threads and/or processes and then multiplying
    that value times the number of threads and processes:

        * It would seem that we would want to just treat each thread and/or
          process as providing a sample

        * In the current way, if each thread runs sequentially in its
          entirety, then this method is equivalent to just running with a
          bigger batch size, with the caveat that it will skew the results due
          to all the context switch overhead between the thread, and then the
          thread/process multiplier will skew the results entirely. If each
          thread runs entirely in parallel with the others, using the min/max
          start/end times along with dividing by the aggregate count of
          operations from all threads/processes and multiplying by those
          processes does normalize the results (time / count_from_all_threads)
          * #_of_all_threads = (time / count_from_one_thread). In either case
          nothing is gained by such aggregation since the same values can be
          arrived at by just treating each result from a thread/process as
          another sample.
